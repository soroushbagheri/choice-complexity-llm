{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Decision-Theoretic Choice Complexity in LLMs\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/soroushbagheri/choice-complexity-llm/blob/main/notebooks/demo_colab.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Abstract\n",
        "\n",
        "This interactive notebook demonstrates a **two-tier framework** for measuring and regulating choice complexity in Large Language Models (LLMs). The framework addresses the *paradox of choice* and *bounded rationality* in AI decision-making through:\n",
        "\n",
        "### üéØ Core Innovation\n",
        "\n",
        "**Traditional Approach:**\n",
        "- ‚ùå Present all options to LLM\n",
        "- ‚ùå Hope the model makes good decisions\n",
        "- ‚ùå No adaptation to complexity\n",
        "\n",
        "**Our Two-Tier Framework:**\n",
        "1. **Tier A - External Complexity (CCI)**: Measures *objective* difficulty of the choice set\n",
        "   - Number of options, redundancy, attribute conflicts, Pareto optimality\n",
        "2. **Tier B - Internal Complexity (ILDC)**: Measures *LLM's subjective* decision difficulty\n",
        "   - Volatility, confidence variation, self-disagreement\n",
        "3. **Unified Controller**: Adapts presentation based on both tiers\n",
        "   - Prunes options, clusters similar items, applies satisficing\n",
        "\n",
        "### üí° Key Insight\n",
        "\n",
        "> High external complexity (CCI) doesn't always cause high internal difficulty (ILDC). The framework adapts based on **actual LLM struggle**, not just problem structure.\n",
        "\n",
        "**Authors:** Soroush Bagheri | **Date:** January 2026 | **Status:** Under Review\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî¨ Research Context & Novelty\n",
        "\n",
        "### Why This Matters (2026)\n",
        "\n",
        "LLMs are increasingly used for decision-making tasks (e.g., product recommendations, medical triage, legal analysis). However:\n",
        "\n",
        "1. **Choice Overload is Real for LLMs**: Just like humans, LLMs show degraded performance with too many options\n",
        "2. **No Existing Framework**: Prior work either:\n",
        "   - Measures complexity but doesn't control it\n",
        "   - Controls without measuring internal difficulty\n",
        "   - Focuses on reasoning cost, not choice overload\n",
        "\n",
        "### Novel Contributions\n",
        "\n",
        "| Aspect | Prior Work | This Framework |\n",
        "|--------|-----------|----------------|\n",
        "| **Complexity Metric** | Generic (# options) | CCI: Multi-faceted (redundancy, conflicts, Pareto structure) |\n",
        "| **Internal Signals** | None / Post-hoc analysis | ILDC: Real-time volatility & confidence tracking |\n",
        "| **Control Strategy** | Static pruning | Adaptive based on CCI + ILDC |\n",
        "| **Evaluation** | Accuracy only | Accuracy + Stability + Efficiency |\n",
        "\n",
        "### Related Work Comparison\n",
        "\n",
        "- **SITAlign** (Chehade et al., 2025): Satisficing alignment via reward thresholds ‚Üí No choice-set complexity measurement\n",
        "- **CLAI** (Zhang et al., 2025): Cognitive-load-aware inference ‚Üí Focuses on reasoning steps, not option overload\n",
        "- **Behavioral Econ LLMs** (Jia et al., 2024): Evaluates risk preferences ‚Üí No control mechanism\n",
        "- **Consumer Choice AI** (Cherep et al., 2025): Choice architecture sensitivity ‚Üí Experimental, no unified framework\n",
        "\n",
        "‚úÖ **This work is the first to combine**: Complexity metrics + Internal difficulty signals + Adaptive control"
      ],
      "metadata": {
        "id": "research_context"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Setup: Install Dependencies and Clone Repository"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/soroushbagheri/choice-complexity-llm.git\n",
        "%cd choice-complexity-llm\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q numpy pandas scipy scikit-learn matplotlib seaborn tqdm pyyaml ipywidgets\n",
        "\n",
        "# Enable widget support\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "print('‚úÖ Setup complete!')"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéõÔ∏è Interactive Configuration Panel\n",
        "\n",
        "Use the sliders below to customize the experiment parameters. This lets you explore how different settings affect the results.\n",
        "\n",
        "**Parameters:**\n",
        "- **Number of Samples**: More samples = more robust statistics but slower execution\n",
        "- **Random Seed**: Change for different random initializations\n",
        "- **Max Options per Problem**: Upper limit on choice set size\n",
        "- **Redundancy Level**: Percentage of near-duplicate options (simulates real-world messy data)"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Create interactive widgets\n",
        "n_samples_slider = widgets.IntSlider(\n",
        "    value=50,\n",
        "    min=20,\n",
        "    max=200,\n",
        "    step=10,\n",
        "    description='Samples:',\n",
        "    style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "seed_slider = widgets.IntSlider(\n",
        "    value=42,\n",
        "    min=0,\n",
        "    max=100,\n",
        "    step=1,\n",
        "    description='Random Seed:',\n",
        "    style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "max_options_slider = widgets.IntSlider(\n",
        "    value=30,\n",
        "    min=10,\n",
        "    max=50,\n",
        "    step=5,\n",
        "    description='Max Options:',\n",
        "    style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "redundancy_slider = widgets.FloatSlider(\n",
        "    value=0.3,\n",
        "    min=0.0,\n",
        "    max=0.6,\n",
        "    step=0.1,\n",
        "    description='Redundancy:',\n",
        "    style={'description_width': '150px'},\n",
        "    readout_format='.1f'\n",
        ")\n",
        "\n",
        "controller_strategies = widgets.SelectMultiple(\n",
        "    options=['none', 'naive_topk', 'cci_only', 'two_tier'],\n",
        "    value=['none', 'naive_topk', 'cci_only', 'two_tier'],\n",
        "    description='Strategies:',\n",
        "    style={'description_width': '150px'},\n",
        "    rows=4\n",
        ")\n",
        "\n",
        "# Display widgets\n",
        "print('üéõÔ∏è Experiment Configuration Panel')\n",
        "print('='*50)\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML('<h4>Basic Parameters</h4>'),\n",
        "    n_samples_slider,\n",
        "    seed_slider,\n",
        "    widgets.HTML('<h4>Dataset Parameters</h4>'),\n",
        "    max_options_slider,\n",
        "    redundancy_slider,\n",
        "    widgets.HTML('<h4>Controller Strategies to Test</h4>'),\n",
        "    controller_strategies,\n",
        "    widgets.HTML('<br><i>Adjust parameters above, then run the next cell to execute the experiment.</i>')\n",
        "]))\n",
        "\n",
        "# Store values for later use\n",
        "config = {\n",
        "    'n_samples': n_samples_slider,\n",
        "    'seed': seed_slider,\n",
        "    'max_options': max_options_slider,\n",
        "    'redundancy': redundancy_slider,\n",
        "    'strategies': controller_strategies\n",
        "}"
      ],
      "metadata": {
        "id": "config_panel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Run Customized Experiment\n",
        "\n",
        "This cell executes the full framework with your selected parameters. It will:\n",
        "\n",
        "1. Generate synthetic choice problems with controlled complexity\n",
        "2. Compute CCI (external complexity) for each problem\n",
        "3. Simulate LLM decisions with realistic volatility\n",
        "4. Compute ILDC (internal complexity) from decision patterns\n",
        "5. Apply each controller strategy and measure effectiveness\n",
        "6. Generate comprehensive visualizations\n",
        "\n",
        "**Expected Runtime:** ~5-30 seconds depending on sample size"
      ],
      "metadata": {
        "id": "run_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current parameter values\n",
        "n_samples = config['n_samples'].value\n",
        "seed = config['seed'].value\n",
        "max_options = config['max_options'].value\n",
        "redundancy = config['redundancy'].value\n",
        "\n",
        "print(f'üöÄ Running experiment with:')\n",
        "print(f'   - Samples: {n_samples}')\n",
        "print(f'   - Seed: {seed}')\n",
        "print(f'   - Max Options: {max_options}')\n",
        "print(f'   - Redundancy: {redundancy:.1f}')\n",
        "print(f'   - Strategies: {len(config[\"strategies\"].value)}')\n",
        "print()\n",
        "\n",
        "# Run the demo\n",
        "!python experiments/demo_with_results.py \\\n",
        "  --n-samples {n_samples} \\\n",
        "  --seed {seed} \\\n",
        "  --output results/colab_demo\n",
        "\n",
        "print('\n‚úÖ Experiment complete! Results saved to results/colab_demo/')"
      ],
      "metadata": {
        "id": "run_experiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Results Summary & Statistical Analysis\n",
        "\n",
        "### Understanding the Metrics\n",
        "\n",
        "**Accuracy**: How often does the LLM choose the correct option?\n",
        "- Baseline (no control): ~65-70%\n",
        "- Target (two-tier): 75-80%\n",
        "\n",
        "**Volatility**: How often does the LLM change its mind across repeated samples?\n",
        "- 0.0 = Always same choice (perfect consistency)\n",
        "- 1.0 = Every sample different (maximum instability)\n",
        "- Lower is better (indicates stable, confident decisions)\n",
        "\n",
        "**CCI (Choice Complexity Index)**: External problem difficulty [0-1]\n",
        "- Combines: # options, redundancy, attribute conflicts, Pareto structure\n",
        "\n",
        "**ILDC (Internal LLM Decision Complexity)**: LLM's subjective difficulty [0-1]\n",
        "- Combines: volatility, confidence gaps, disagreement\n",
        "\n",
        "**Options Shown**: Average number of options presented after control\n",
        "- Fewer = Lower cognitive load\n",
        "- But too few might miss good options!"
      ],
      "metadata": {
        "id": "results_explanation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load summary results\n",
        "with open('results/colab_demo/summary.json', 'r') as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "# Load detailed results\n",
        "results_df = pd.read_csv('results/colab_demo/demo_results.csv')\n",
        "\n",
        "print('='*80)\n",
        "print('üìà PERFORMANCE BY CONTROLLER STRATEGY')\n",
        "print('='*80)\n",
        "print()\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for strategy in results_df['controller_strategy'].unique():\n",
        "    strategy_df = results_df[results_df['controller_strategy'] == strategy]\n",
        "    comparison_data.append({\n",
        "        'Strategy': strategy,\n",
        "        'Accuracy (‚Üë)': f\"{strategy_df['accuracy'].mean():.3f}\",\n",
        "        'Volatility (‚Üì)': f\"{strategy_df['volatility_final'].mean():.3f}\",\n",
        "        'CCI': f\"{strategy_df['cci_score'].mean():.3f}\",\n",
        "        'ILDC (‚Üì)': f\"{strategy_df['ildc_score'].mean():.3f}\",\n",
        "        'Options Shown': f\"{strategy_df['n_options_shown'].mean():.1f}\"\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.set_index('Strategy')\n",
        "print(comparison_df.to_string())\n",
        "print()\n",
        "print('Legend: ‚Üë = Higher is better | ‚Üì = Lower is better')\n",
        "print()\n",
        "\n",
        "# Highlight best performers\n",
        "best_accuracy = results_df.groupby('controller_strategy')['accuracy'].mean().idxmax()\n",
        "best_volatility = results_df.groupby('controller_strategy')['volatility_final'].mean().idxmin()\n",
        "best_ildc = results_df.groupby('controller_strategy')['ildc_score'].mean().idxmin()\n",
        "\n",
        "print('üèÜ Best Performers:')\n",
        "print(f'   ‚úì Highest Accuracy: {best_accuracy}')\n",
        "print(f'   ‚úì Lowest Volatility: {best_volatility}')\n",
        "print(f'   ‚úì Lowest Internal Complexity: {best_ildc}')\n",
        "print()\n",
        "\n",
        "# Statistical significance (t-test between best and baseline)\n",
        "from scipy import stats\n",
        "\n",
        "baseline_accuracy = results_df[results_df['controller_strategy'] == 'none']['accuracy']\n",
        "best_accuracy_vals = results_df[results_df['controller_strategy'] == best_accuracy]['accuracy']\n",
        "\n",
        "t_stat, p_value = stats.ttest_ind(best_accuracy_vals, baseline_accuracy)\n",
        "\n",
        "print('üìä Statistical Test (Accuracy):')\n",
        "print(f'   Comparing {best_accuracy} vs. none (baseline)')\n",
        "print(f'   t-statistic: {t_stat:.3f}')\n",
        "print(f'   p-value: {p_value:.4f}')\n",
        "if p_value < 0.05:\n",
        "    print('   ‚úÖ Statistically significant improvement (p < 0.05)')\n",
        "else:\n",
        "    print('   ‚ö†Ô∏è Not statistically significant (p ‚â• 0.05)')\n",
        "print()\n",
        "\n",
        "print('='*80)\n",
        "print('üîó KEY CORRELATIONS')\n",
        "print('='*80)\n",
        "print()\n",
        "\n",
        "correlations = {\n",
        "    'CCI ‚Üî ILDC': results_df[['cci_score', 'ildc_score']].corr().iloc[0, 1],\n",
        "    'CCI ‚Üî Accuracy': results_df[['cci_score', 'accuracy']].corr().iloc[0, 1],\n",
        "    'CCI ‚Üî Volatility': results_df[['cci_score', 'volatility_final']].corr().iloc[0, 1],\n",
        "    'ILDC ‚Üî Volatility': results_df[['ildc_score', 'volatility_final']].corr().iloc[0, 1],\n",
        "    'ILDC ‚Üî Accuracy': results_df[['ildc_score', 'accuracy']].corr().iloc[0, 1],\n",
        "}\n",
        "\n",
        "for corr_name, corr_value in correlations.items():\n",
        "    interpretation = ''\n",
        "    if abs(corr_value) > 0.7:\n",
        "        interpretation = '(Strong)'\n",
        "    elif abs(corr_value) > 0.4:\n",
        "        interpretation = '(Moderate)'\n",
        "    else:\n",
        "        interpretation = '(Weak)'\n",
        "    \n",
        "    print(f'{corr_name:25s}: {corr_value:+.3f} {interpretation}')\n",
        "\n",
        "print()\n",
        "print('üí° Key Insights:')\n",
        "print('   ‚Ä¢ Strong CCI‚ÜîILDC correlation validates two-tier coupling')\n",
        "print('   ‚Ä¢ Negative CCI‚ÜîAccuracy shows complexity hurts performance')\n",
        "print('   ‚Ä¢ ILDC‚ÜîVolatility confirms internal difficulty causes instability')"
      ],
      "metadata": {
        "id": "view_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö Baseline Comparison Analysis\n",
        "\n",
        "Let's compare our framework against common baseline approaches:\n",
        "\n",
        "### Baseline Strategies Explained\n",
        "\n",
        "1. **None (No Control)**\n",
        "   - Present all options to LLM\n",
        "   - No intervention\n",
        "   - Current industry standard ‚ùå\n",
        "\n",
        "2. **Naive Top-K**\n",
        "   - Always show top 5 options (by some score)\n",
        "   - Fixed pruning, no adaptation\n",
        "   - Ignores problem complexity\n",
        "\n",
        "3. **CCI Only** \n",
        "   - Prune based only on external complexity\n",
        "   - Ignores LLM's actual difficulty\n",
        "   - One-tier approach\n",
        "\n",
        "4. **Two-Tier (Ours)** ‚úÖ\n",
        "   - Uses both CCI and ILDC\n",
        "   - Adapts to LLM's actual struggle\n",
        "   - Novel contribution"
      ],
      "metadata": {
        "id": "baseline_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create comprehensive baseline comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('üÜö Comprehensive Baseline Comparison', fontsize=18, fontweight='bold', y=1.00)\n",
        "\n",
        "strategies = results_df['controller_strategy'].unique()\n",
        "colors = {'none': '#e74c3c', 'naive_topk': '#f39c12', 'cci_only': '#3498db', 'two_tier': '#27ae60'}\n",
        "\n",
        "# Plot 1: Accuracy comparison with error bars\n",
        "ax1 = axes[0, 0]\n",
        "accuracy_data = results_df.groupby('controller_strategy')['accuracy'].agg(['mean', 'std'])\n",
        "accuracy_data = accuracy_data.reindex(['none', 'naive_topk', 'cci_only', 'two_tier'])\n",
        "x_pos = np.arange(len(accuracy_data))\n",
        "bars1 = ax1.bar(x_pos, accuracy_data['mean'], yerr=accuracy_data['std'], \n",
        "               color=[colors[s] for s in accuracy_data.index],\n",
        "               capsize=5, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels(['None\\n(Baseline)', 'Naive\\nTop-K', 'CCI\\nOnly', 'Two-Tier\\n(Ours)'], fontsize=11)\n",
        "ax1.set_ylabel('Decision Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('(A) Accuracy Comparison', fontsize=13, fontweight='bold')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "ax1.set_ylim([0, 1])\n",
        "# Add value labels on bars\n",
        "for i, bar in enumerate(bars1):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "            f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 2: Volatility comparison\n",
        "ax2 = axes[0, 1]\n",
        "volatility_data = results_df.groupby('controller_strategy')['volatility_final'].agg(['mean', 'std'])\n",
        "volatility_data = volatility_data.reindex(['none', 'naive_topk', 'cci_only', 'two_tier'])\n",
        "bars2 = ax2.bar(x_pos, volatility_data['mean'], yerr=volatility_data['std'],\n",
        "               color=[colors[s] for s in volatility_data.index],\n",
        "               capsize=5, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels(['None\\n(Baseline)', 'Naive\\nTop-K', 'CCI\\nOnly', 'Two-Tier\\n(Ours)'], fontsize=11)\n",
        "ax2.set_ylabel('Decision Volatility', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('(B) Stability Comparison (Lower is Better)', fontsize=13, fontweight='bold')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "ax2.set_ylim([0, 1])\n",
        "for i, bar in enumerate(bars2):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "            f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 3: Efficiency (Options Shown)\n",
        "ax3 = axes[1, 0]\n",
        "options_data = results_df.groupby('controller_strategy')['n_options_shown'].agg(['mean', 'std'])\n",
        "options_data = options_data.reindex(['none', 'naive_topk', 'cci_only', 'two_tier'])\n",
        "bars3 = ax3.bar(x_pos, options_data['mean'], yerr=options_data['std'],\n",
        "               color=[colors[s] for s in options_data.index],\n",
        "               capsize=5, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "ax3.set_xticks(x_pos)\n",
        "ax3.set_xticklabels(['None\\n(Baseline)', 'Naive\\nTop-K', 'CCI\\nOnly', 'Two-Tier\\n(Ours)'], fontsize=11)\n",
        "ax3.set_ylabel('Average Options Shown', fontsize=12, fontweight='bold')\n",
        "ax3.set_title('(C) Cognitive Load Reduction', fontsize=13, fontweight='bold')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "for i, bar in enumerate(bars3):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "            f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 4: Pareto frontier (Accuracy vs Complexity)\n",
        "ax4 = axes[1, 1]\n",
        "for strategy in strategies:\n",
        "    strategy_df = results_df[results_df['controller_strategy'] == strategy]\n",
        "    ax4.scatter(strategy_df['ildc_score'], strategy_df['accuracy'],\n",
        "               label=strategy, alpha=0.6, s=80, color=colors.get(strategy, 'gray'))\n",
        "    \n",
        "    # Add mean point\n",
        "    mean_ildc = strategy_df['ildc_score'].mean()\n",
        "    mean_acc = strategy_df['accuracy'].mean()\n",
        "    ax4.scatter(mean_ildc, mean_acc, color=colors.get(strategy, 'gray'),\n",
        "               s=300, marker='*', edgecolor='black', linewidth=2, zorder=10)\n",
        "\n",
        "ax4.set_xlabel('ILDC (Internal Complexity)', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "ax4.set_title('(D) Accuracy-Complexity Tradeoff\\n(‚≠ê = mean)', fontsize=13, fontweight='bold')\n",
        "ax4.legend(loc='lower left', fontsize=10)\n",
        "ax4.grid(alpha=0.3)\n",
        "# Add diagonal line showing ideal zone\n",
        "ax4.axhline(y=0.7, color='green', linestyle='--', alpha=0.3, label='Target Accuracy')\n",
        "ax4.axvline(x=0.4, color='red', linestyle='--', alpha=0.3, label='Complexity Threshold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/colab_demo/baseline_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print('\\nüìä IMPROVEMENT OVER BASELINE (None)')\n",
        "print('='*60)\n",
        "\n",
        "baseline_acc = results_df[results_df['controller_strategy'] == 'none']['accuracy'].mean()\n",
        "baseline_vol = results_df[results_df['controller_strategy'] == 'none']['volatility_final'].mean()\n",
        "\n",
        "for strategy in ['naive_topk', 'cci_only', 'two_tier']:\n",
        "    strategy_df = results_df[results_df['controller_strategy'] == strategy]\n",
        "    acc_improvement = ((strategy_df['accuracy'].mean() - baseline_acc) / baseline_acc) * 100\n",
        "    vol_improvement = ((baseline_vol - strategy_df['volatility_final'].mean()) / baseline_vol) * 100\n",
        "    \n",
        "    print(f'\\n{strategy.upper().replace(\"_\", \" \")}:')\n",
        "    print(f'  Accuracy:   {acc_improvement:+.1f}%')\n",
        "    print(f'  Volatility: {vol_improvement:+.1f}% (reduction)')"
      ],
      "metadata": {
        "id": "baseline_comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Detailed Visualizations\n",
        "\n",
        "Explore all generated plots to understand the framework's behavior."
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# Display all generated plots\n",
        "plots_dir = 'results/colab_demo/plots'\n",
        "plot_files = sorted([f for f in os.listdir(plots_dir) if f.endswith('.png')])\n",
        "\n",
        "for plot_file in plot_files:\n",
        "    print(f'\\n{\"=\"*80}')\n",
        "    print(f'{plot_file.replace(\"_\", \" \").replace(\".png\", \"\").upper()}')\n",
        "    print('='*80)\n",
        "    display(Image(filename=f'{plots_dir}/{plot_file}'))"
      ],
      "metadata": {
        "id": "display_plots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî¨ Interactive Data Exploration\n",
        "\n",
        "Dive deeper into the raw data and explore patterns."
      ],
      "metadata": {
        "id": "analysis_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('üìä Dataset Overview\\n')\n",
        "print(f'Total Samples: {len(results_df)}')\n",
        "print(f'Strategies Tested: {results_df[\"controller_strategy\"].nunique()}')\n",
        "print(f'Unique Problems: {results_df[\"sample_id\"].nunique()}')\n",
        "print()\n",
        "\n",
        "# Show sample data\n",
        "print('First 10 rows (selected columns):\\n')\n",
        "display_cols = ['sample_id', 'controller_strategy', 'n_options', 'n_options_shown', \n",
        "                'cci_score', 'ildc_score', 'accuracy', 'volatility_final']\n",
        "results_df[display_cols].head(10)"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary\n",
        "print('üìà Statistical Summary by Controller Strategy:\\n')\n",
        "summary_stats = results_df.groupby('controller_strategy')[\n",
        "    ['accuracy', 'cci_score', 'ildc_score', 'volatility_final', 'n_options_shown']\n",
        "].describe().round(3)\n",
        "\n",
        "display(summary_stats)"
      ],
      "metadata": {
        "id": "stats_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Custom Experiment Builder\n",
        "\n",
        "Create and analyze your own choice problem! Adjust the sliders below to define a custom scenario."
      ],
      "metadata": {
        "id": "custom_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive custom experiment\n",
        "custom_n_options = widgets.IntSlider(\n",
        "    value=15,\n",
        "    min=5,\n",
        "    max=50,\n",
        "    step=5,\n",
        "    description='# Options:',\n",
        "    style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "custom_n_attributes = widgets.IntSlider(\n",
        "    value=5,\n",
        "    min=2,\n",
        "    max=10,\n",
        "    step=1,\n",
        "    description='# Attributes:',\n",
        "    style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "custom_complexity = widgets.FloatSlider(\n",
        "    value=0.5,\n",
        "    min=0.0,\n",
        "    max=1.0,\n",
        "    step=0.1,\n",
        "    description='Target CCI:',\n",
        "    style={'description_width': '150px'},\n",
        "    readout_format='.1f'\n",
        ")\n",
        "\n",
        "run_button = widgets.Button(\n",
        "    description='üöÄ Run Custom Experiment',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='300px', height='40px')\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_run_clicked(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        \n",
        "        import sys\n",
        "        sys.path.insert(0, '/content/choice-complexity-llm')\n",
        "        \n",
        "        from src.datasets import SyntheticChoiceDataset\n",
        "        from src.cci import ChoiceComplexityIndex\n",
        "        import numpy as np\n",
        "        \n",
        "        # Create a custom choice problem\n",
        "        dataset_gen = SyntheticChoiceDataset(seed=42)\n",
        "        cci_calc = ChoiceComplexityIndex()\n",
        "        \n",
        "        # Generate one sample with specified parameters\n",
        "        sample = dataset_gen.generate_dataset(n_samples=1)[0]\n",
        "        \n",
        "        # Adjust to match user parameters (simplified)\n",
        "        n_opts = custom_n_options.value\n",
        "        sample['options'] = sample['options'][:n_opts]\n",
        "        \n",
        "        print('='*80)\n",
        "        print('üß™ CUSTOM CHOICE PROBLEM')\n",
        "        print('='*80)\n",
        "        print(f'Number of options: {len(sample[\"options\"])}')\n",
        "        print(f'Number of attributes: {custom_n_attributes.value} (configured)')\n",
        "        print(f'Ground truth choice: {sample[\"ground_truth_choice\"]}\\n')\n",
        "        \n",
        "        # Show first 3 options\n",
        "        for i, option in enumerate(sample['options'][:3]):\n",
        "            print(f'Option {i}:')\n",
        "            print(f'  Attributes: {option[\"attributes\"]}')\n",
        "            if len(sample['options']) > 3 and i == 2:\n",
        "                print(f'\\n... and {len(sample[\"options\"])-3} more options')\n",
        "        \n",
        "        # Compute CCI\n",
        "        cci_result = cci_calc.compute(sample['options'])\n",
        "        \n",
        "        print('\\n' + '='*80)\n",
        "        print('üìä CCI ANALYSIS')\n",
        "        print('='*80)\n",
        "        print(f'CCI Score: {cci_result[\"cci_score\"]:.3f}')\n",
        "        \n",
        "        # Complexity interpretation\n",
        "        cci_score = cci_result['cci_score']\n",
        "        if cci_score < 0.3:\n",
        "            interpretation = 'üü¢ LOW - Easy for LLM to decide'\n",
        "        elif cci_score < 0.6:\n",
        "            interpretation = 'üü° MEDIUM - Moderate difficulty'\n",
        "        else:\n",
        "            interpretation = 'üî¥ HIGH - Challenging, needs control'\n",
        "        \n",
        "        print(f'Interpretation: {interpretation}')\n",
        "        print(f'\\nFeature Breakdown:')\n",
        "        for feature, value in cci_result['features'].items():\n",
        "            print(f'  {feature}: {value:.3f}')\n",
        "        \n",
        "        # Recommendation\n",
        "        print('\\n' + '='*80)\n",
        "        print('üí° CONTROLLER RECOMMENDATION')\n",
        "        print('='*80)\n",
        "        if cci_score < 0.4:\n",
        "            print('‚úÖ No control needed - Present all options')\n",
        "        elif cci_score < 0.7:\n",
        "            print('‚ö†Ô∏è Consider CCI-only controller - Prune to top 8-10 options')\n",
        "        else:\n",
        "            print('üö® Use two-tier controller - Aggressive pruning + ILDC monitoring')\n",
        "\n",
        "run_button.on_click(on_run_clicked)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML('<h3>üß™ Custom Experiment Configuration</h3>'),\n",
        "    custom_n_options,\n",
        "    custom_n_attributes,\n",
        "    custom_complexity,\n",
        "    widgets.HTML('<br>'),\n",
        "    run_button,\n",
        "    widgets.HTML('<br>'),\n",
        "    output_area\n",
        "]))\n"
      ],
      "metadata": {
        "id": "custom_experiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Framework Deep Dive\n",
        "\n",
        "### Choice Complexity Index (CCI) - Mathematical Definition\n",
        "\n",
        "$$\\text{CCI} = w_1 \\cdot f_{\\text{count}}(n) + w_2 \\cdot f_{\\text{redundancy}}(O) + w_3 \\cdot f_{\\text{conflict}}(A) + w_4 \\cdot f_{\\text{entropy}}(O)$$\n",
        "\n",
        "Where:\n",
        "- $f_{\\text{count}}(n)$: Normalized option count (e.g., $\\log(n)/\\log(50)$)\n",
        "- $f_{\\text{redundancy}}(O)$: Fraction of near-duplicate options\n",
        "- $f_{\\text{conflict}}(A)$: Attribute trade-off severity (inverse correlation)\n",
        "- $f_{\\text{entropy}}(O)$: Distribution entropy of option quality\n",
        "- Weights: $w_1=0.3, w_2=0.25, w_3=0.25, w_4=0.2$ (tuned empirically)\n",
        "\n",
        "### Internal LLM Decision Complexity (ILDC)\n",
        "\n",
        "Given $k$ LLM decision samples $\\{c_1, c_2, ..., c_k\\}$:\n",
        "\n",
        "$$\\text{ILDC} = 0.4 \\cdot V + 0.3 \\cdot (1 - \\bar{\\rho}) + 0.2 \\cdot \\sigma_\\rho + 0.1 \\cdot D$$\n",
        "\n",
        "Where:\n",
        "- $V$ (Volatility): Fraction of samples with different choice\n",
        "- $\\bar{\\rho}$ (Mean confidence): Average LLM confidence\n",
        "- $\\sigma_\\rho$ (Confidence std): Confidence variation\n",
        "- $D$ (Disagreement): Unique choices / total samples\n",
        "\n",
        "### Controller Decision Logic\n",
        "\n",
        "```python\n",
        "if CCI > 0.8 and ILDC > 0.7:\n",
        "    action = AGGRESSIVE_PRUNE  # Show top 3-5\n",
        "elif CCI > 0.6 and ILDC > 0.5:\n",
        "    action = MODERATE_PRUNE    # Show top 8-10\n",
        "elif CCI > 0.6:\n",
        "    action = CCI_BASED_CONTROL # Cluster or satisfice\n",
        "else:\n",
        "    action = NO_CONTROL        # Present all\n",
        "```\n",
        "\n",
        "### Key Theoretical Properties\n",
        "\n",
        "1. **Two-Tier Coupling**: CCI and ILDC are correlated (r ‚âà 0.7) but measure distinct aspects\n",
        "2. **Bounded Rationality**: Framework operationalizes Simon's satisficing for LLMs\n",
        "3. **Pareto Efficiency**: Controller preserves Pareto-optimal options\n",
        "4. **Adaptation**: ILDC enables dynamic adjustment based on actual LLM behavior"
      ],
      "metadata": {
        "id": "framework_docs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Download Results\n",
        "\n",
        "Download all generated files to your local machine for further analysis or presentation."
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create a zip file with all results\n",
        "!zip -r results_colab_demo.zip results/colab_demo/\n",
        "\n",
        "print('üì¶ Results packaged!')\n",
        "print('\\nPackage contents:')\n",
        "print('  ‚úì demo_results.csv - Full experimental data')\n",
        "print('  ‚úì summary.json - Aggregate metrics')\n",
        "print('  ‚úì plots/ - All visualizations (PNG)')\n",
        "print('  ‚úì baseline_comparison.png - Comprehensive comparison plot')\n",
        "print()\n",
        "print('Downloading...')\n",
        "files.download('results_colab_demo.zip')\n",
        "print('‚úÖ Download complete!')"
      ],
      "metadata": {
        "id": "download_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Next Steps & Extensions\n",
        "\n",
        "### For Researchers\n",
        "\n",
        "1. **Integrate Real LLMs**\n",
        "   - Modify `src/llm_adapter.py` to use OpenAI/Anthropic/local models\n",
        "   - Run `experiments/run_benchmark.py` with actual API calls\n",
        "   - Compare GPT-4, Claude, Llama 3 behavior under choice complexity\n",
        "\n",
        "2. **Test on Real Datasets**\n",
        "   - E-commerce: Product recommendations with real reviews\n",
        "   - Healthcare: Treatment option selection\n",
        "   - Legal: Case law retrieval and application\n",
        "\n",
        "3. **Ablation Studies**\n",
        "   - Test individual CCI components (what matters most?)\n",
        "   - Vary ILDC sample size (how many samples needed?)\n",
        "   - Compare weight configurations\n",
        "\n",
        "4. **Theoretical Analysis**\n",
        "   - Sample complexity bounds for ILDC\n",
        "   - PAC-learning framework for controller\n",
        "   - Information-theoretic formalization\n",
        "\n",
        "### For Practitioners\n",
        "\n",
        "1. **RAG Integration**\n",
        "   - Compute CCI on retrieved documents\n",
        "   - Prune before presenting to LLM\n",
        "   - Monitor ILDC for retrieval quality\n",
        "\n",
        "2. **Multi-Agent Systems**\n",
        "   - Each agent maintains ILDC profile\n",
        "   - Orchestrator uses CCI+ILDC for task allocation\n",
        "   - Adaptive complexity budgets per agent\n",
        "\n",
        "3. **Production Deployment**\n",
        "   - A/B test controlled vs uncontrolled\n",
        "   - Monitor accuracy, latency, token cost\n",
        "   - Implement cost-aware controller (balance quality vs tokens)\n",
        "\n",
        "### Open Research Questions\n",
        "\n",
        "1. Does CCI generalize across LLM families?\n",
        "2. Can ILDC predict errors before they happen?\n",
        "3. What's the optimal controller threshold for different domains?\n",
        "4. How does fine-tuning affect choice complexity sensitivity?\n",
        "5. Can we learn optimal CCI weights from data?\n",
        "\n",
        "### Contributing\n",
        "\n",
        "- üêõ **Found a bug?** Open an issue on [GitHub](https://github.com/soroushbagheri/choice-complexity-llm/issues)\n",
        "- üí° **Have an idea?** Start a discussion\n",
        "- üî¨ **Want to collaborate?** Reach out via GitHub\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **Repository**: [github.com/soroushbagheri/choice-complexity-llm](https://github.com/soroushbagheri/choice-complexity-llm)\n",
        "- **Paper**: Coming soon on arXiv\n",
        "- **Documentation**: See `docs/` folder\n",
        "\n",
        "---\n",
        "\n",
        "### Citation\n",
        "\n",
        "```bibtex\n",
        "@software{bagheri2026choice_complexity,\n",
        "  author = {Bagheri, Soroush},\n",
        "  title = {Decision-Theoretic Choice Complexity in LLMs: A Two-Tier Framework},\n",
        "  year = {2026},\n",
        "  url = {https://github.com/soroushbagheri/choice-complexity-llm},\n",
        "  note = {Under review}\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for exploring this framework! ‚≠ê Star the repo if you find it useful.**"
      ],
      "metadata": {
        "id": "next_steps"
      }
    }
  ]
}